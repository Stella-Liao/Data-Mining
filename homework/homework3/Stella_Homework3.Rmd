---
title: "STA 545 Statistical Data Mining I, Fall 2020"
subtitle: "Homework 3"
author: "Stella Liao"
date: "September 23, 2020"
output:
  pdf_document:
            latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. (40 points) This problem involves the Boston data set, which we discussed in the data analysis example about subset selection. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
library(MASS)
data(Boston)
```

### (a) For each predictor, ﬁt a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically signiﬁcant association between the predictor and the response?    

1) The simple linear regression model between 'crim' and 'zn'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.zn = lm(crim ~ zn, Boston)
summary(simple.lm.fit.zn)
```

2) The simple linear regression model between 'crim' and 'indus'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.indus = lm(crim ~ indus, Boston)
summary(simple.lm.fit.indus)
```

3) The simple linear regression model between 'crim' and 'chas'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.chas = lm(crim ~ chas, Boston)
summary(simple.lm.fit.chas)
```

4) The simple linear regression model between 'crim' and 'nox'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.nox = lm(crim ~ nox, Boston)
summary(simple.lm.fit.nox)
```

5) The simple linear regression model between 'crim' and 'rm'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.rm = lm(crim ~ rm, Boston)
summary(simple.lm.fit.rm)
```

6) The simple linear regression model between 'crim' and 'age'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.age = lm(crim ~ age, Boston)
summary(simple.lm.fit.age)
```

7) The simple linear regression model between 'crim' and 'dis'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.dis = lm(crim ~ dis, Boston)
summary(simple.lm.fit.dis)
```

8) The simple linear regression model between 'crim' and 'rad'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.rad = lm(crim ~ rad, Boston)
summary(simple.lm.fit.rad)
```

9) The simple linear regression model between 'crim' and 'tax'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.tax = lm(crim ~ tax, Boston)
summary(simple.lm.fit.tax)
```

10) The simple linear regression model between 'crim' and 'ptratio'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.ptratio = lm(crim ~ ptratio, Boston)
summary(simple.lm.fit.ptratio)
```

11) The simple linear regression model between 'crim' and 'black'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.black = lm(crim ~ black, Boston)
summary(simple.lm.fit.black)
```

12) The simple linear regression model between 'crim' and 'lstat'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.lstat = lm(crim ~ lstat, Boston)
summary(simple.lm.fit.lstat)
```

13) The simple linear regression model between 'crim' and 'medv'
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
simple.lm.fit.medv = lm(crim ~ medv, Boston)
summary(simple.lm.fit.medv)
```

Based on the p-values of each model in the chunks above, we could know that, except the predictor 'chas', other variables all have a statistically significant association with the response 'crim'.  

Moreover, the p-values of the variable 'indus', 'nox', 'dis', 'rad', 'tax', 'black', 'lstat', and 'medv' are all smaller than 2.2e-16, which is very close to 0, meaning that there is a much more statistically signiﬁcant association between the predictor and the response in those models.


### (b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H~0~ : $\beta$~j~ = 0

```{r echo=TRUE}
multiple.lm.fit = lm(crim ~., Boston)
summary(multiple.lm.fit)
```
Based on the p-values of each variable in the multiple regression model in the chunk above, we could know that, for the predictors 'dis', 'rad', 'medv', 'zn', 'black', 'nox' and 'lstat', we could reject the null hypothesis because their p-values are small enough. Specifically,  
when $\alpha$ = 0.001, for the predictors 'dis' and 'rad', we could reject the null hypothesis ;  

when $\alpha$ = 0.01, besides the predictors above, for the predictors 'medv', we could reject the null hypothesis;  

when $\alpha$ = 0.05, besides the predictors above, for the predictors 'zn' and 'black', we could reject the null hypothesis;  

when $\alpha$ = 0.1, besides the predictors above, for the predictors 'nox' and 'lstat', we could reject the null hypothesis.  

### (c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coeﬃcients from (a) on the x-axis, and the multiple regression coeﬃcients from (b) on the y-axis. 

```{r echo=TRUE}
simple.coefs <- c() 
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.zn)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.indus)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.chas)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.nox)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.rm)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.age)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.dis)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.rad)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.tax)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.ptratio)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.black)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.lstat)[2])
simple.coefs <- append(simple.coefs,coef(simple.lm.fit.medv)[2])

multiple.coefs <- coef(multiple.lm.fit)[2:14]
plot(x = simple.coefs, y = multiple.coefs, 
     xlab = "The univariate regression coeffocients", 
     ylab = "The multiple regression coeffocients",
     main = "The coeffocients of the univariate and the multiple regression",)

```

### (d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, ﬁt a model of the form Y = $\beta$~0~ + $\beta$~1~X + $\beta$~2~X^2^ + $\beta$~3~X^3^ + $\epsilon$, and test H~0~ : $\beta$~2~ = $\beta$~3~ = 0.

1) The polynomial regression model between 'crim' and 'zn'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.zn = lm(crim ~ zn + I(zn^2) + I(zn^3), Boston)
summary(poly.lm.fit.zn)
```

2) The polynomial regression model between 'crim' and 'indus'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.indus = lm(crim ~ indus + I(indus^2) + I(indus^3), Boston)
summary(poly.lm.fit.indus)
```

3) The polynomial regression model between 'crim' and 'chas'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.chas = lm(crim ~ chas + I(chas^2) + I(chas^3), Boston)
summary(poly.lm.fit.chas)
```

4) The polynomial regression model between 'crim' and 'nox'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.nox = lm(crim ~ nox + I(nox^2) + I(nox^3), Boston)
summary(poly.lm.fit.nox)
```

5) The polynomial regression model between 'crim' and 'rm'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.rm = lm(crim ~ rm + I(rm^2) + I(rm^3), Boston)
summary(poly.lm.fit.rm)
```

6) The polynomial regression model between 'crim' and 'age'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.age = lm(crim ~ age + I(age^2) + I(age^3), Boston)
summary(poly.lm.fit.age)
```

7) The polynomial regression model between 'crim' and 'dis'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.dis = lm(crim ~ dis + I(dis^2) + I(dis^3), Boston)
summary(poly.lm.fit.dis)
```

8) The polynomial regression model between 'crim' and 'rad'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.rad = lm(crim ~ rad + I(rad^2) + I(rad^3), Boston)
summary(poly.lm.fit.rad)
```

9) The polynomial regression model between 'crim' and 'tax'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.tax = lm(crim ~ tax + I(tax^2) + I(tax^3), Boston)
summary(poly.lm.fit.tax)
```

10) The polynomial regression model between 'crim' and 'ptratio'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.ptratio = lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3), Boston)
summary(poly.lm.fit.ptratio)
```

11) The polynomial regression model between 'crim' and 'black'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.black = lm(crim ~ black + I(black^2) + I(black^3), Boston)
summary(poly.lm.fit.black)
```

12) The polynomial regression model between 'crim' and 'lstat'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.lstat = lm(crim ~ lstat + I(lstat^2) + I(lstat^3), Boston)
summary(poly.lm.fit.lstat)
```

13) The polynomial regression model between 'crim' and 'medv'

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
poly.lm.fit.medv = lm(crim ~ medv + I(medv^2) + I(medv^3), Boston)
summary(poly.lm.fit.medv)
```

For the variables 'indus', 'nox', 'age', 'dis', 'ptratio', and 'medv', we could reject the null hypothesis that H~0~: $\beta$~2~= $\beta$~3~= 0, because squared and cubed terms of each model of these variables are statistically significant, which means there are evidence of a non-linear relationship with those variables.  

And for the remaining variables, so far, there is no evidence of a non-linear relationship between the predictor and outcome variables.


## 2. (30 points) We perform best subset, forward selection, and backward elimination selection on a single data set. For each approach, we obtain p+1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:

### (a) Which of the three models with k predictors has the smallest training error?

The model with best subset selection has the smallest training error because it considers every possible model with k predictors.

### (b) Which of the three models with k predictors has the smallest test error?

It depends. Best subset selection might have the smallest test error because it will consider more models than other methods. However, other methods might pick a model with smaller test error by luck.

### (c) True or False:

#### (i) The predictors in the k-variable model identiﬁed by forward selection are a subset of the predictors in the (k+1)-variable model identiﬁed by forward selection.  

True

#### (ii) The predictors in the k-variable model identiﬁed by backward elimination are a subset of the predictors in the (k + 1)variable model identiﬁed by backward elimination.  

True

#### (iii) The predictors in the k-variable model identiﬁed by backward elimination are a subset of the predictors in the (k + 1)variable model identiﬁed by forward selection.  

False

#### (iv) The predictors in the k-variable model identiﬁed by forward selection are a subset of the predictors in the (k+1)-variable model identiﬁed by backward elimination.  

False

#### (v) The predictors in the k-variable model identiﬁed by best subset are a subset of the predictors in the (k + 1)-variable model identiﬁed by best subset selection.  

False

## 3. (30 points) In this exercise, we will generate simulated data, and will then use this data to perform best subset selection, forward selection, backward elimination. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.

### (a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector of $\epsilon$ length n = 100.

```{r}
set.seed(1)
X <- rnorm(100)
epsilon <- rnorm(100)
```

### (b) Generate a response vector Y of length n = 100 according to the model Y = 1 + 2.5X + 2X^2^ + X^3^ + $\epsilon$.
```{r}
Y <- 1 + 2.5 * X + 2 * X^2 + X^3 + epsilon
```

### (c) Use the regsubsets() function to perform best subset selection, forward selection, and backward elimination in order to choose the best model containing the predictors X, X^2^ , ..., X^10^ . Report the coeﬃcients of the selected models.

1) best subset selection
```{r}
library(leaps)
data1 <- data.frame(Y, X)
regfit.full <- regsubsets(Y ~ poly(X, 10), data1, nvmax = 10, method = "exhaustive")
reg.summary <- summary(regfit.full)


which.min(reg.summary$bic)
which.min(reg.summary$cp)
which.max(reg.summary$adjr2)
```
2) Forward stepwise selection 
```{r}
regfit.fwd <- regsubsets(Y ~ poly(X, 10), data1, nvmax = 10, method = "forward")
reg.summary.fwd <- summary(regfit.fwd)

which.min(reg.summary.fwd$bic)
which.min(reg.summary.fwd$cp)
which.max(reg.summary.fwd$adjr2)
```

1) Backward stepwise selection
```{r}
regfit.bwd <- regsubsets(Y ~ poly(X, 10), data1, nvmax = 10, method = "backward")
reg.summary.bwd <- summary(regfit.bwd)

which.min(reg.summary.bwd$bic)
which.min(reg.summary.bwd$cp)
which.max(reg.summary.bwd$adjr2)
```

According to the outputs of the chunks above, we could find that, for best subset selection, with BIC, we choose  the 3-variables model; with C~p~, we choose the 4-variables model, and with adjusted R^2^ we choose the 5-variables model. And with forward stepwise selection and backward stepwise selection, the results are same with those when using best subset selection.

Therefore, I choose 4-variable model with best subeset selection as the best model. The coefficients are shown below. 
```{r}
coef(regfit.full, which.min(reg.summary$cp))
```

